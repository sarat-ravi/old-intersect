<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.4//EN"
"http://www.oasis-open.org/docbook/xml/4.4/docbookx.dtd"[
]>
<!--
Licensed to the Apache Software Foundation (ASF) under one
or more contributor license agreements.  See the NOTICE file
distributed with this work for additional information
regarding copyright ownership.  The ASF licenses this file
to you under the Apache License, Version 2.0 (the
"License"); you may not use this file except in compliance
with the License.  You may obtain a copy of the License at

   http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing,
software distributed under the License is distributed on an
"AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
KIND, either express or implied.  See the License for the
specific language governing permissions and limitations
under the License.
-->

<chapter id="tools.corpora">

	<title>Corpora</title>
	<para>
	OpenNLP has built-in support to convert various corpora
	into the native training format needed by the different
	trainable components.
	</para>
	<section id="tools.corpora.conll">
		<title>CONLL</title>
		<para>
		CoNLL stands for the Confernece on Computational Natural Language Learning and is not
		a single project but a consortium of developers attempting to broaden the computing
		environment. More information about the entire conference series can be obtained here
		for CoNLL.
		</para>
		<section id="tools.corpora.conll.2000">
		<title>CONLL 2000</title>
		<para>
		The shared task of CoNLL-2000 is Chunking .
		</para>
		<section id="tools.corpora.conll.2000.getting">
		<title>Getting the data</title>
		<para>
		CoNLL-2000 made available training and test data for the Chunk task in English. 
		The data consists of the same partitions of the Wall Street Journal corpus (WSJ) 
		as the widely used data for noun phrase chunking: sections 15-18 as training data 
		(211727 tokens) and section 20 as test data (47377 tokens). The annotation of the 
		data has been derived from the WSJ corpus by a program written by Sabine Buchholz 
		from Tilburg University, The Netherlands. Both training and test data can be
		obtained from <ulink url="http://www.cnts.ua.ac.be/conll2000/chunking">http://www.cnts.ua.ac.be/conll2000/chunking</ulink>. 
		</para>
		</section>
		<section id="tools.corpora.conll.2000.converting">
		<title>Converting the data</title>
		<para>
		The data don't need to be transformed because Apache OpenNLP Chunker follows
		the CONLL 2000 format for training. Check <link linkend="tools.chunker.training">Chunker Training</link> section to learn more.
		</para>
		</section>
		<section id="tools.corpora.conll.2000.training">
		<title>Training</title>
		<para>
		 We can train the model for the Chunker using the train.txt available at CONLL 2000:
		 <programlisting>
			<![CDATA[
bin/opennlp ChunkerTrainerME -encoding UTF-8 -lang en -iterations 500 \
-data train.txt -model en-chunker.bin]]>
		</programlisting>
		<programlisting>
			<![CDATA[
Indexing events using cutoff of 5

	Computing event counts...  done. 211727 events
	Indexing...  done.
Sorting and merging events... done. Reduced 211727 events to 197252.
Done indexing.
Incorporating indexed data for training...  
done.
	Number of Event Tokens: 197252
	    Number of Outcomes: 22
	  Number of Predicates: 107838
...done.
Computing model parameters...
Performing 500 iterations.
  1:  .. loglikelihood=-654457.1455212828	0.2601510435608118
  2:  .. loglikelihood=-239513.5583724216	0.9260037690044255
  3:  .. loglikelihood=-141313.1386347238	0.9443387003074715
  4:  .. loglikelihood=-101083.50853437989	0.954375209585929
... cut lots of iterations ...
498:  .. loglikelihood=-1710.8874647317095	0.9995040783650645
499:  .. loglikelihood=-1708.0908900815848	0.9995040783650645
500:  .. loglikelihood=-1705.3045902366732	0.9995040783650645
Writing chunker model ... done (4.019s)

Wrote chunker model to path: .\en-chunker.bin]]>
		</programlisting>
		</para>
		</section>
		<section id="tools.corpora.conll.2000.evaluation">
		<title>Evaluating</title>
		<para>
		We evaluate the model using the file test.txt  available at CONLL 2000:
		<programlisting>
			<![CDATA[
$ bin/opennlp ChunkerEvaluator -encoding utf8 -model en-chunker.bin -data test.txt]]>
		</programlisting>
		<programlisting>
			<![CDATA[
Loading Chunker model ... done (0,665s)
current: 85,8 sent/s avg: 85,8 sent/s total: 86 sent
current: 88,1 sent/s avg: 87,0 sent/s total: 174 sent
current: 156,2 sent/s avg: 110,0 sent/s total: 330 sent
current: 192,2 sent/s avg: 130,5 sent/s total: 522 sent
current: 167,2 sent/s avg: 137,8 sent/s total: 689 sent
current: 179,2 sent/s avg: 144,6 sent/s total: 868 sent
current: 183,2 sent/s avg: 150,3 sent/s total: 1052 sent
current: 183,2 sent/s avg: 154,4 sent/s total: 1235 sent
current: 169,2 sent/s avg: 156,0 sent/s total: 1404 sent
current: 178,2 sent/s avg: 158,2 sent/s total: 1582 sent
current: 172,2 sent/s avg: 159,4 sent/s total: 1754 sent
current: 177,2 sent/s avg: 160,9 sent/s total: 1931 sent


Average: 161,6 sent/s 
Total: 2013 sent
Runtime: 12.457s

Precision: 0.9244354736974896
Recall: 0.9216837162502096
F-Measure: 0.9230575441395671]]>
		</programlisting>
		</para>
		</section>
	</section>
		<section id="tools.corpora.conll.2003">
		<title>CONLL 2002</title>
		<para>
		TODO: Document how to use the converters for CONLL 2002. Any contributions
		are very welcome. If you want to contribute please contact us on the mailing list
		or comment on the jira issue 
		<ulink url="https://issues.apache.org/jira/browse/OPENNLP-46">OPENNLP-46</ulink>.
		</para>
		</section>
		<section id="tools.corpora.conll.2003">
		<title>CONLL 2003</title>
		<para>
		The shared task of CoNLL-2003 is language independent named entity recognition
		for English and German.
		</para>
		<section id="tools.corpora.conll.2003.getting">
		<title>Getting the data</title>
		<para>
		The English data is the Reuters Corpus, which is a collection of news wire articles.
		The Reuters Corpus can be obtained free of charges from the NIST for research
		purposes: <ulink url="http://trec.nist.gov/data/reuters/reuters.html">http://trec.nist.gov/data/reuters/reuters.html</ulink>
		</para>
		<para>
		The German data is a collection of articles from the German newspaper Frankfurter
		Rundschau. The articles are part of the ECI Multilingual Text Corpus which
		can be obtained for 75$ (2010) from the Linguistic Data Consortium:
<ulink url="http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC94T5">http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC94T5</ulink>		</para>
		<para>After one of the corpora is available the data must be
		transformed as explained in the README file to the conll format.
		The transformed data can be read by the OpenNLP CONLL03 converter.
		</para>
		</section>
		<section id="tools.corpora.conll.2003.converting">
		<title>Converting the data</title>
		<para>
		To convert the information to the OpenNLP format:
		<programlisting>
			<![CDATA[
$ bin/opennlp TokenNameFinderConverter conll03 -data eng.train -lang en -types per > corpus_train.txt]]>
		</programlisting>
		Optionally, you can convert the training test samples as well.
		<programlisting>
			<![CDATA[
bin/opennlp TokenNameFinderConverter conll03 -data eng.testa -lang en -types per > corpus_testa.txt
bin/opennlp TokenNameFinderConverter conll03 -data eng.testb -lang en -types per > corpus_testb.txt]]>
		</programlisting>
		</para>
		</section>
		<section id="tools.corpora.conll.2003.training.english">
		<title>Training with English data</title>
		<para>
		 To train the model for the name finder:
		 <programlisting>
			<![CDATA[
$ bin/opennlp TokenNameFinderTrainer -lang en -encoding utf8 -iterations 500 \
    -data corpus_train.txt -model en_ner_person.bin]]>
		</programlisting>
		<programlisting>
			<![CDATA[
Indexing events using cutoff of 5

	Computing event counts...  done. 203621 events
	Indexing...  done.
Sorting and merging events... done. Reduced 203621 events to 179409.
Done indexing.
Incorporating indexed data for training...  
done.
	Number of Event Tokens: 179409
	    Number of Outcomes: 3
	  Number of Predicates: 58814
...done.
Computing model parameters...
Performing 500 iterations.
  1:  .. loglikelihood=-223700.5328318588	0.9453494482396216
  2:  .. loglikelihood=-40525.939777363084	0.9467933071736215
  3:  .. loglikelihood=-24893.98837874921	0.9598518816821447
  4:  .. loglikelihood=-18420.3379471033	0.9712996203731442
... cut lots of iterations ...
498:  .. loglikelihood=-952.8501399442295	0.9988950059178572
499:  .. loglikelihood=-952.0600155746948	0.9988950059178572
500:  .. loglikelihood=-951.2722802086295	0.9988950059178572
Writing name finder model ... done (1.638s)

Wrote name finder model to
path: .\en_ner_person.bin]]>
		</programlisting>
		</para>
		</section>
		<section id="tools.corpora.conll.2003.evaluation.english">
		<title>Evaluating with English data</title>
		<para>
		Since we created the test A and B files above, we can use them to evaluate the model.
		<programlisting>
			<![CDATA[
$ bin/opennlp TokenNameFinderEvaluator -lang en -encoding utf8 -model en_ner_person.bin \
    -data corpus_testa.txt]]>
		</programlisting>
		<programlisting>
			<![CDATA[
Loading Token Name Finder model ... done (0.359s)
current: 190.2 sent/s avg: 190.2 sent/s total: 199 sent
current: 648.3 sent/s avg: 415.9 sent/s total: 850 sent
current: 530.1 sent/s avg: 453.6 sent/s total: 1380 sent
current: 793.8 sent/s avg: 539.0 sent/s total: 2178 sent
current: 705.4 sent/s avg: 571.9 sent/s total: 2882 sent


Average: 569.4 sent/s
Total: 3251 sent
Runtime: 5.71s

Precision: 0.9366247297154147
Recall: 0.739956568946797
F-Measure: 0.8267557582133971]]>
		</programlisting>
		</para>
		</section>
	</section>
	</section>
	<section id="tools.corpora.arvores-deitadas">
		<title>Arvores Deitadas</title>
		<para>
		The Portuguese corpora available at <ulink url="http://www.linguateca.pt">Floresta Sintá(c)tica</ulink> project follow the Arvores Deitadas (AD) format. Apache OpenNLP includes tools to convert from AD format to native format.  
		</para>		
		<section id="tools.corpora.arvores-deitadas.getting">
			<title>Getting the data</title>
			<para>
			The Corpus can be downloaded from here: <ulink url="http://www.linguateca.pt/floresta/corpus.html">http://www.linguateca.pt/floresta/corpus.html</ulink>
			</para>
			<para>
			The Name Finder models were trained using the Amazonia corpus: <ulink url="http://www.linguateca.pt/floresta/ficheiros/gz/amazonia.ad.gz">amazonia.ad</ulink>.
			The Chunker models were trained using the <ulink url="http://www.linguateca.pt/floresta/ficheiros/gz/Bosque_CF_8.0.ad.txt.gz">Bosque_CF_8.0.ad</ulink>.
			</para>
		</section>
		
		<section id="tools.corpora.arvores-deitadas.converting">
			<title>Converting the data</title>
			<para>
				To extract NameFinder training data from Amazonia corpus:
			<programlisting>
			<![CDATA[
$ bin/opennlp TokenNameFinderConverter ad -encoding ISO-8859-1 -data amazonia.ad \
    -lang pt -types per > corpus.txt]]>
			</programlisting>
			</para>
			<para>
				To extract Chunker training data from Bosque_CF_8.0.ad corpus:
			<programlisting>
			<![CDATA[
$ bin/opennlp ChunkerConverter ad -encoding ISO-8859-1 -data Bosque_CF_8.0.ad.txt > bosque-chunk]]>
			</programlisting>
			</para>
		</section>
		<section id="tools.corpora.arvores-deitadas.evaluation">
			<title>Evaluation</title>
			<para>
			To perform the evaluation the corpus was split into a training and a test part.
			<programlisting>
			<![CDATA[
$ sed '1,55172d' corpus.txt > corpus_train.txt
$ sed '55172,100000000d' corpus.txt > corpus_test.txt]]>
			</programlisting>
			<programlisting>
			<![CDATA[
$ bin/opennlp TokenNameFinderTrainer -lang PT -encoding UTF-8 -data corpus_train.txt \
    -model pt-ner.bin -cutoff 20
..
$ bin/opennlp TokenNameFinderEvaluator -encoding UTF-8 -model ../model/pt-ner.bin \
    -data corpus_test.txt

Precision: 0.8005071889818507
Recall: 0.7450581122145297
F-Measure: 0.7717879983140168]]>
			</programlisting>
			</para>
		</section>
	</section>
	<section id="tools.corpora.leipzig">
	<title>Leipzig Corpora</title>
	<para>
	The Leiopzig Corpora collection presents corpora in different languages. The corpora is a collection of individual sentences collected
	from the web and newspapers. The Corpora is available as plain text and as MySQL database tables. The OpenNLP integration can only
	use the plain text version.
	</para>
	<para>
	The corpora in the different languages can be used to train a document categorizer model which can detect the document language. 
	The	individual plain text packages can be downlaoded here:
	<ulink url="http://corpora.uni-leipzig.de/download.html">http://corpora.uni-leipzig.de/download.html</ulink>
	</para>
	
	<para>
	Afer all packages have been downloaded, unzip them and use the following commands to
	produce a training file which can be processed by the Document Categorizer:
	<programlisting>
			<![CDATA[
bin/opennlp DoccatConverter leipzig -lang cat -data Leipzig/cat100k/sentences.txt >> lang.train
bin/opennlp DoccatConverter leipzig -lang de -data Leipzig/de100k/sentences.txt >> lang.train
bin/opennlp DoccatConverter leipzig -lang dk -data Leipzig/dk100k/sentences.txt >> lang.train
bin/opennlp DoccatConverter leipzig -lang ee -data Leipzig/ee100k/sentences.txt >> lang.train
bin/opennlp DoccatConverter leipzig -lang en -data Leipzig/en100k/sentences.txt >> lang.train
bin/opennlp DoccatConverter leipzig -lang fi -data Leipzig/fi100k/sentences.txt >> lang.train
bin/opennlp DoccatConverter leipzig -lang fr -data Leipzig/fr100k/sentences.txt >> lang.train
bin/opennlp DoccatConverter leipzig -lang it -data Leipzig/it100k/sentences.txt >> lang.train
bin/opennlp DoccatConverter leipzig -lang jp -data Leipzig/jp100k/sentences.txt >> lang.train
bin/opennlp DoccatConverter leipzig -lang kr -data Leipzig/kr100k/sentences.txt >> lang.train
bin/opennlp DoccatConverter leipzig -lang nl -data Leipzig/nl100k/sentences.txt >> lang.train
bin/opennlp DoccatConverter leipzig -lang no -data Leipzig/no100k/sentences.txt >> lang.train
bin/opennlp DoccatConverter leipzig -lang se -data Leipzig/se100k/sentences.txt >> lang.train
bin/opennlp DoccatConverter leipzig -lang sorb -data Leipzig/sorb100k/sentences.txt >> lang.train
bin/opennlp DoccatConverter leipzig -lang tr -data Leipzig/tr100k/sentences.txt >> lang.train]]>
	</programlisting>
	</para>
	<para>
	Depending on your platform local it might be problemmatic to output characters which are not supported by that encoding,
	we suggest to run these command on a platform which has a unicode default encoding, e.g. Linux with UTF-8.
	</para>
	<para>
	Afer the lang.train file is created the actual language detection document categorizer model
	can be created with the following command.
	<programlisting>
			<![CDATA[
bin/opennlp DoccatTrainer -lang x-unspecified -encoding MacRoman -data ../lang.train -model lang.model
Indexing events using cutoff of 5

	Computing event counts...  done. 10000 events
	Indexing...  done.
Sorting and merging events... done. Reduced 10000 events to 10000.
Done indexing.
Incorporating indexed data for training...  
done.
	Number of Event Tokens: 10000
	    Number of Outcomes: 2
	  Number of Predicates: 42730
...done.
Computing model parameters...
Performing 100 iterations.
  1:  .. loglikelihood=-6931.471805600547	0.5
  2:  .. loglikelihood=-2110.9654348555955	1.0
... cut lots of iterations ...

 99:  .. loglikelihood=-0.449640418555347	1.0
100:  .. loglikelihood=-0.443746359746235	1.0
Writing document categorizer model ... done (1.210s)

Wrote document categorizer model to
path: /Users/joern/dev/opennlp-apache/opennlp/opennlp-tools/lang.model
]]>
	</programlisting>
	In the sample above the language detection model was trained to distinguish two languages, danish and english.
	</para>
	
	<para>
	After the model is created it can be used to detect the two languages:
	
	<programlisting>
			<![CDATA[
$ bin/opennlp Doccat ../lang.
lang.model  lang.train  
karkand:opennlp-tools joern$ bin/opennlp Doccat ../lang.model
Loading Document Categorizer model ... done (0.289s)
The American Finance Association is pleased to announce the award of ...
en	The American Finance Association is pleased to announce the award of ..
.
Danskerne skal betale for den økonomiske krise ved at blive længere på arbejdsmarkedet .
dk	Danskerne skal betale for den økonomiske krise ved at blive længere på arbejdsmarkedet .]]>	
	</programlisting>
	</para>
	</section>
</chapter>